{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentium Take Home Test\n",
    "\n",
    "## Challenge Specification\n",
    "\n",
    "1. Create a simple convolutional network (with 3-5 total layers) to classify MNIST dataset.\n",
    "2. Set up the training and validation flow and train the network on the MNIST dataset.\n",
    "3. Calculate the network classification performance on the validation and training datasets.\n",
    "4. Calculate the network size in Kbytes.\n",
    "5. Perform an 8-bit post-training quantization of the trained network and recalculate the network\n",
    "size in KB and its classification performance.\n",
    "6. Perform an 8-bit quantization-aware training on the original network and recalculate the network\n",
    "size in KB and its classification performance after the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations and Constraints\n",
    "\n",
    "1. I am opting to use ready-made library to handle the dataset.\n",
    "2. I am opting to use PyTorch as required to build the model.\n",
    "3. All quantization will be done using PyTorch quantization tools only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "# Quantization\n",
    "import torch.ao.quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool_pad = nn.MaxPool2d(2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.forward(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = MNIST(root='data', download=True, transform=transforms_)\n",
    "# dataset_train, dataset_test = MNIST(root='data', download=True, train=True), MNIST(root='data', download=True, train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Split\n",
    "kfold = KFold(n_splits=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals to store the best model\n",
    "best_params = {\n",
    "    'model': None,\n",
    "    'accuracy': 0,\n",
    "    'epoch': 0,\n",
    "    'fold': 0,\n",
    "    'training_acc': None,\n",
    "    'val_acc': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.0995897501707077\n",
      "Accuracy: 0.4885333333333333\n",
      "Fold 1\n",
      "Epoch 0\n",
      "Loss: -0.10104354470968246\n",
      "Accuracy: 0.5833\n",
      "Best Model : \n",
      "{\n",
      "    \"accuracy\": 0,\n",
      "    \"epoch\": 0,\n",
      "    \"fold\": 1,\n",
      "    \"training_acc\": -0.602523684501648,\n",
      "    \"val_acc\": 0.5833\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Updated train and test functions with kfold\n",
    "\n",
    "import torch.utils.data.dataloader\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "def reset_model_params(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def train(model, train_loader, loss_fn, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(train_loader):\n",
    "            X, y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val = loss.item()\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Loss: {loss_val}\")\n",
    "                \n",
    "    return loss_val\n",
    "        \n",
    "def validate(model, test_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model.forward(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "    val_loss = loss.item()\n",
    "    return val_loss\n",
    "\n",
    "def test_acc(model, test_loader, device):\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model.predict(X)\n",
    "            correct += torch.sum(y_pred == y).item()\n",
    "            total += y.size(0)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold}\")\n",
    "    sample_train, sample_test = torch.utils.data.SubsetRandomSampler(train_idx), torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=sample_train)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=sample_test)\n",
    "    \n",
    "    model = MNISTClassifier()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.apply(reset_model_params)\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loss_val = train(model, train_loader, loss_fn, device=device)\n",
    "        val_loss = validate(model, test_loader, loss_fn=loss_fn, device=device)\n",
    "        val_acc = test_acc(model, test_loader, device=device)\n",
    "      \n",
    "        if best_params['val_acc'] is None or val_acc > best_params['val_acc'] :\n",
    "            best_params['model'] = model\n",
    "            best_params['epoch'] = epoch\n",
    "            best_params['fold'] = fold\n",
    "            best_params['training_acc'] = loss_val\n",
    "            best_params['val_acc'] = val_acc  \n",
    "\n",
    "print(\"Best Model : \")\n",
    "print(json.dumps({k:best_params[k] for k in best_params.keys() if 'model' not in k}, indent=4))\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_params['model'].state_dict(), f\"checkpoints/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Size Calculation\n",
    "\n",
    "1. The network size will be calculated using the `torchsummary` library. While this is not extremely difficult to implement, I am opting to use the library to save time.\n",
    "2. The network size will be calculated in KBytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "summary = torchinfo.summary(model, input_size = (1, 1, 28, 28))\n",
    "\n",
    "# network_fp32_size = summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MNISTClassifier                          [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 32, 28, 28]           320\n",
       "├─ReLU: 1-2                              [1, 32, 28, 28]           --\n",
       "├─MaxPool2d: 1-3                         [1, 32, 15, 15]           --\n",
       "├─Conv2d: 1-4                            [1, 64, 15, 15]           18,496\n",
       "├─ReLU: 1-5                              [1, 64, 15, 15]           --\n",
       "├─MaxPool2d: 1-6                         [1, 64, 8, 8]             --\n",
       "├─Conv2d: 1-7                            [1, 128, 8, 8]            73,856\n",
       "├─ReLU: 1-8                              [1, 128, 8, 8]            --\n",
       "├─MaxPool2d: 1-9                         [1, 128, 4, 4]            --\n",
       "├─Linear: 1-10                           [1, 128]                  262,272\n",
       "├─Linear: 1-11                           [1, 10]                   1,290\n",
       "├─Softmax: 1-12                          [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 356,234\n",
       "Trainable params: 356,234\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 9.40\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.38\n",
       "Params size (MB): 1.42\n",
       "Estimated Total Size (MB): 1.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size :  1395.39453125 Kbytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Model file size : \", os.path.getsize('checkpoints/model_0_0.pth') / 1024, \"Kbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "### Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model needs to be redeffined and loaded from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thameem/data1/workspace/mentium_takehome/venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "# NOTE: This is not working as expected due to pytorch expecting the dequant to precede the softmax layer \n",
    "# class MNISTClassificationPostTrainingStatic(MNISTClassifier):\n",
    "#     def __init__(self, model_file):\n",
    "#         super(MNISTClassificationPostTrainingStatic, self).__init__()\n",
    "#         self.quant = torch.ao.quantization.QuantStub()\n",
    "#         self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "#         self.model = MNISTClassifier()\n",
    "#         self.model.load_state_dict(torch.load(model_file))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.quant(x)\n",
    "#         x = self.model(x)\n",
    "#         x = self.dequant(x)\n",
    "#         return x\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         with torch.no_grad():\n",
    "#             return torch.argmax(self.forward(x), 1)\n",
    "\n",
    "class MNISTClassificationQuantize(nn.Module):\n",
    "    def __init__(self, weight_path) -> None:\n",
    "        super(MNISTClassificationQuantize, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU() # Cannot use the same relu layer as the quantization expects the relu to be fused with the conv layer\n",
    "        \n",
    "        self.maxpool_pad = nn.MaxPool2d(2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        \n",
    "        self.load_state_dict(torch.load(weight_path))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dequant(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.forward(x), 1)\n",
    "        \n",
    "model_fp32 = MNISTClassificationQuantize('checkpoints/model_0_0.pth')\n",
    "\n",
    "model_fp32.eval()\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(\n",
    "    model_fp32, [\n",
    "        ['conv1', 'relu1'], \n",
    "        ['conv2', 'relu2'], \n",
    "        ['conv3', 'relu3']\n",
    "        ])\n",
    "model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "inp_fp32 = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "model_fp32_prepared(inp_fp32)\n",
    "\n",
    "model_fp32_quantized = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "res = model_fp32_quantized(inp_fp32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_fp32,\n",
    "    # torch.randn(1, 1, 28, 28).cuda() if torch.cuda.is_available() else torch.randn(1, 1, 28, 28),\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_fp32_additional.onnx',\n",
    "    export_params=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_fp32_quantized,\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_fp32_quantized.onnx',\n",
    "    export_params=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT - Quantization Aware Training\n",
    "\n",
    "As with the post-training quantization, I will use the PyTorch quantization tools to perform the quantization aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thameem/data1/workspace/mentium_takehome/venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Finetune Epoch : 0\n",
      "Loss: -0.568603515625\n",
      "Accuracy: 0.5833666666666667\n",
      "Best Model : \n",
      "{\n",
      "    \"accuracy\": 0,\n",
      "    \"epoch\": 0,\n",
      "    \"fold\": 1,\n",
      "    \"training_acc\": -0.6180827021598816,\n",
      "    \"val_acc\": 0.5833666666666667\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Quantization Aware Training\n",
    "\n",
    "model_fp32 = MNISTClassificationQuantize('checkpoints/best_model.pth')\n",
    "model_fp32.train()\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(\n",
    "    model_fp32, [\n",
    "        ['conv1', 'relu1'], \n",
    "        ['conv2', 'relu2'], \n",
    "        ['conv3', 'relu3']\n",
    "        ])\n",
    "model_fp32_qat_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "QAT_FINE_TUNE_EPOCHS = 1\n",
    "\n",
    "qat_best_params = {\n",
    "    'model': None,\n",
    "    'accuracy': 0,\n",
    "    'epoch': 0,\n",
    "    'fold': 0,\n",
    "    'training_acc': None,\n",
    "    'val_acc': None\n",
    "}\n",
    "\n",
    "qat_device = torch.device('cpu') # Choosing CPU for now\n",
    "\n",
    "# K Fold not needed as we are using the best model\n",
    "for epoch in range(QAT_FINE_TUNE_EPOCHS):\n",
    "    print(f\"QAT Finetune Epoch : {epoch}\")\n",
    "    loss_val = train(model_fp32_qat_prepared, train_loader, loss_fn, device=qat_device)\n",
    "    val_loss = validate(model_fp32_qat_prepared, test_loader, loss_fn=loss_fn, device=qat_device)\n",
    "    val_acc = test_acc(model_fp32_qat_prepared, test_loader, device=qat_device)\n",
    "    \n",
    "    if qat_best_params['val_acc'] is None or val_acc > qat_best_params['val_acc'] :\n",
    "        qat_best_params['model'] = model_fp32_qat_prepared\n",
    "        qat_best_params['epoch'] = epoch\n",
    "        qat_best_params['fold'] = fold\n",
    "        qat_best_params['training_acc'] = loss_val\n",
    "        qat_best_params['val_acc'] = val_acc\n",
    "        \n",
    "print(\"Best Model : \")\n",
    "print(json.dumps({k:qat_best_params[k] for k in qat_best_params.keys() if 'model' not in k}, indent=4))\n",
    "\n",
    "# Save the best model\n",
    "torch.save(qat_best_params['model'].state_dict(), f\"checkpoints/qat_best_model.pth\")\n",
    "\n",
    "model_fp32_qat_prepared.eval()\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_qat_prepared)\n",
    "\n",
    "inp_fp32 = torch.randn(1, 1, 28, 28)\n",
    "res = model_int8(inp_fp32)\n",
    "\n",
    "# Export ONNX\n",
    "torch.onnx.export(\n",
    "    model_int8,\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_int8.onnx',\n",
    "    export_params=True,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchinfo\n",
    "# torchinfo.summary(model_fp32_quantized, input_size=(1, 1, 28, 28))\n",
    "\n",
    "# TODO: Calculate the size of the model\n",
    "\n",
    "def calculate_model_size(model):\n",
    "    model_size = 0\n",
    "    for param in model.parameters():\n",
    "        print(param.numel(), param.element_size())\n",
    "        model_size += param.numel() * param.element_size()\n",
    "    return model_size\n",
    "\n",
    "\n",
    "# model_fp32_size = calculate_model_size(model_fp32)\n",
    "# model_fp32_quantized_size = calculate_model_size(model_fp32_quantized)\n",
    "model_int8_size = calculate_model_size(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'conv1',\n",
       " 'conv2',\n",
       " 'conv3',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'dequant',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fc1',\n",
       " 'fc2',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'maxpool',\n",
       " 'maxpool_pad',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'predict',\n",
       " 'quant',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'relu1',\n",
       " 'relu2',\n",
       " 'relu3',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'softmax',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model FP32 Size :  1391.5390625 Kbytes\n",
      "Model INT8 Size :  0.0 Kbytes\n"
     ]
    }
   ],
   "source": [
    "# print(\"Model FP32 Size : \", model_fp32_size / 1024, \"Kbytes\")\n",
    "# print(\"Model FP32 Quantized Size : \", model_fp32_quantized_size / 1024, \"Kbytes\")\n",
    "print(\"Model INT8 Size : \", model_int8_size / 1024, \"Kbytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
