{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentium Take Home Test\n",
    "\n",
    "## Challenge Specification\n",
    "\n",
    "1. Create a simple convolutional network (with 3-5 total layers) to classify MNIST dataset.\n",
    "2. Set up the training and validation flow and train the network on the MNIST dataset.\n",
    "3. Calculate the network classification performance on the validation and training datasets.\n",
    "4. Calculate the network size in Kbytes.\n",
    "5. Perform an 8-bit post-training quantization of the trained network and recalculate the network\n",
    "size in KB and its classification performance.\n",
    "6. Perform an 8-bit quantization-aware training on the original network and recalculate the network\n",
    "size in KB and its classification performance after the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations and Constraints\n",
    "\n",
    "1. I am opting to use ready-made library to handle the dataset.\n",
    "2. I am opting to use PyTorch as required to build the model.\n",
    "3. All quantization will be done using PyTorch quantization tools only.\n",
    "4. I am opting to not implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "# Quantization\n",
    "import torch.ao.quantization\n",
    "\n",
    "# Logging\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "FP32_TRAIN_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "CROSS_VALIDATION_FOLDS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "FINE_LOG_INTERVAL = 1000\n",
    "FINE_LOG_OUT = False\n",
    "QUANTIZATION_TRAIN_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool_pad = nn.MaxPool2d(2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.forward(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = MNIST(root='data', download=True, transform=transforms_)\n",
    "\n",
    "# Cross Validation Split\n",
    "kfold = KFold(n_splits=CROSS_VALIDATION_FOLDS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals to store the best model\n",
    "best_params = {\n",
    "    'model': None,\n",
    "    'accuracy': 0,\n",
    "    'epoch': 0,\n",
    "    'fold': 0,\n",
    "    'training_acc': None,\n",
    "    'val_acc': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.497\n",
      "Epoch 1\n",
      "Accuracy: 0.6850833333333334\n",
      "Epoch 2\n",
      "Accuracy: 0.87775\n",
      "Epoch 3\n",
      "Accuracy: 0.9715833333333334\n",
      "Epoch 4\n",
      "Accuracy: 0.9765\n",
      "Epoch 5\n",
      "Accuracy: 0.9801666666666666\n",
      "Epoch 6\n",
      "Accuracy: 0.9824166666666667\n",
      "Epoch 7\n",
      "Accuracy: 0.981\n",
      "Epoch 8\n",
      "Accuracy: 0.9844166666666667\n",
      "Epoch 9\n",
      "Accuracy: 0.984\n",
      "Fold 1\n",
      "Epoch 0\n",
      "Accuracy: 0.69075\n",
      "Epoch 1\n",
      "Accuracy: 0.69925\n",
      "Epoch 2\n",
      "Accuracy: 0.8705833333333334\n",
      "Epoch 3\n",
      "Accuracy: 0.9773333333333334\n",
      "Epoch 4\n",
      "Accuracy: 0.9803333333333333\n",
      "Epoch 5\n",
      "Accuracy: 0.9816666666666667\n",
      "Epoch 6\n",
      "Accuracy: 0.9833333333333333\n",
      "Epoch 7\n",
      "Accuracy: 0.9846666666666667\n",
      "Epoch 8\n",
      "Accuracy: 0.98275\n",
      "Epoch 9\n",
      "Accuracy: 0.983\n",
      "Fold 2\n",
      "Epoch 0\n",
      "Accuracy: 0.5988333333333333\n",
      "Epoch 1\n",
      "Accuracy: 0.661\n",
      "Epoch 2\n",
      "Accuracy: 0.8791666666666667\n",
      "Epoch 3\n",
      "Accuracy: 0.8881666666666667\n",
      "Epoch 4\n",
      "Accuracy: 0.8885\n",
      "Epoch 5\n",
      "Accuracy: 0.9713333333333334\n",
      "Epoch 6\n",
      "Accuracy: 0.98325\n",
      "Epoch 7\n",
      "Accuracy: 0.9805833333333334\n",
      "Epoch 8\n",
      "Accuracy: 0.9843333333333333\n",
      "Epoch 9\n",
      "Accuracy: 0.9873333333333333\n",
      "Fold 3\n",
      "Epoch 0\n",
      "Accuracy: 0.6025\n",
      "Epoch 1\n",
      "Accuracy: 0.6081666666666666\n",
      "Epoch 2\n",
      "Accuracy: 0.9521666666666667\n",
      "Epoch 3\n",
      "Accuracy: 0.9686666666666667\n",
      "Epoch 4\n",
      "Accuracy: 0.97425\n",
      "Epoch 5\n",
      "Accuracy: 0.97825\n",
      "Epoch 6\n",
      "Accuracy: 0.9809166666666667\n",
      "Epoch 7\n",
      "Accuracy: 0.983\n",
      "Epoch 8\n",
      "Accuracy: 0.98025\n",
      "Epoch 9\n",
      "Accuracy: 0.9836666666666667\n",
      "Fold 4\n",
      "Epoch 0\n",
      "Accuracy: 0.594\n",
      "Epoch 1\n",
      "Accuracy: 0.5959166666666667\n",
      "Epoch 2\n",
      "Accuracy: 0.5998333333333333\n",
      "Epoch 3\n",
      "Accuracy: 0.7801666666666667\n",
      "Epoch 4\n",
      "Accuracy: 0.7796666666666666\n",
      "Epoch 5\n",
      "Accuracy: 0.78675\n",
      "Epoch 6\n",
      "Accuracy: 0.7860833333333334\n",
      "Epoch 7\n",
      "Accuracy: 0.78575\n",
      "Epoch 8\n",
      "Accuracy: 0.9769166666666667\n",
      "Epoch 9\n",
      "Accuracy: 0.9796666666666667\n",
      "Best Model : \n",
      "{\n",
      "    \"accuracy\": 0,\n",
      "    \"epoch\": 9,\n",
      "    \"fold\": 2,\n",
      "    \"training_acc\": -0.9846714735031128,\n",
      "    \"val_acc\": 0.9873333333333333\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Updated train and test functions with kfold\n",
    "\n",
    "import torch.utils.data.dataloader\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "def reset_model_params(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def train(model, train_loader, loss_fn, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(train_loader):\n",
    "            X, y = data\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val = loss.item()\n",
    "            \n",
    "            if (i % FINE_LOG_INTERVAL == 0) and FINE_LOG_OUT:\n",
    "                print(f\"Loss: {loss_val}\")\n",
    "                \n",
    "    return loss_val\n",
    "        \n",
    "def validate(model, test_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model.forward(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "    val_loss = loss.item()\n",
    "    return val_loss\n",
    "\n",
    "def test_acc(model, test_loader, device):\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model.predict(X)\n",
    "            correct += torch.sum(y_pred == y).item()\n",
    "            total += y.size(0)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold}\")\n",
    "    sample_train, sample_test = torch.utils.data.SubsetRandomSampler(train_idx), torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=sample_train)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=sample_test)\n",
    "    \n",
    "    model = MNISTClassifier()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.apply(reset_model_params)\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(FP32_TRAIN_EPOCHS):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loss_val = train(model, train_loader, loss_fn, device=device)\n",
    "        val_loss = validate(model, test_loader, loss_fn=loss_fn, device=device)\n",
    "        val_acc = test_acc(model, test_loader, device=device)\n",
    "      \n",
    "        if best_params['val_acc'] is None or val_acc > best_params['val_acc'] :\n",
    "            best_params['model'] = model\n",
    "            best_params['epoch'] = epoch\n",
    "            best_params['fold'] = fold\n",
    "            best_params['training_loss'] = loss_val\n",
    "            best_params['val_loss'] = val_loss\n",
    "            best_params['val_acc'] = val_acc  \n",
    "\n",
    "print(\"Best Model : \")\n",
    "print(json.dumps({k:best_params[k] for k in best_params.keys() if 'model' not in k}, indent=4))\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_params['model'].state_dict(), f\"checkpoints/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Size Calculation\n",
    "\n",
    "1. The network size will be calculated using the `torchsummary` library. While this is not extremely difficult to implement, I am opting to use the library to save time.\n",
    "2. The network size will be calculated in KBytes.\n",
    "\n",
    "NOTE: This library does not support quantized models. The calculation functions is implemented later in the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MNISTClassifier                          [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [1, 32, 28, 28]           320\n",
      "├─ReLU: 1-2                              [1, 32, 28, 28]           --\n",
      "├─MaxPool2d: 1-3                         [1, 32, 15, 15]           --\n",
      "├─Conv2d: 1-4                            [1, 64, 15, 15]           18,496\n",
      "├─ReLU: 1-5                              [1, 64, 15, 15]           --\n",
      "├─MaxPool2d: 1-6                         [1, 64, 8, 8]             --\n",
      "├─Conv2d: 1-7                            [1, 128, 8, 8]            73,856\n",
      "├─ReLU: 1-8                              [1, 128, 8, 8]            --\n",
      "├─MaxPool2d: 1-9                         [1, 128, 4, 4]            --\n",
      "├─Linear: 1-10                           [1, 128]                  262,272\n",
      "├─Linear: 1-11                           [1, 10]                   1,290\n",
      "├─Softmax: 1-12                          [1, 10]                   --\n",
      "==========================================================================================\n",
      "Total params: 356,234\n",
      "Trainable params: 356,234\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 9.40\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 1.42\n",
      "Estimated Total Size (MB): 1.81\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "summary = torchinfo.summary(model, input_size = (1, 1, 28, 28))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "### Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thameem/data1/workspace/mentium_takehome/venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "# NOTE: This is not working as expected due to pytorch expecting the dequant to precede the softmax layer \n",
    "# class MNISTClassificationPostTrainingStatic(MNISTClassifier):\n",
    "#     def __init__(self, model_file):\n",
    "#         super(MNISTClassificationPostTrainingStatic, self).__init__()\n",
    "#         self.quant = torch.ao.quantization.QuantStub()\n",
    "#         self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "#         self.model = MNISTClassifier()\n",
    "#         self.model.load_state_dict(torch.load(model_file))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.quant(x)\n",
    "#         x = self.model(x)\n",
    "#         x = self.dequant(x)\n",
    "#         return x\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         with torch.no_grad():\n",
    "#             return torch.argmax(self.forward(x), 1)\n",
    "\n",
    "class MNISTClassificationQuantize(nn.Module):\n",
    "    def __init__(self, weight_path) -> None:\n",
    "        super(MNISTClassificationQuantize, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU() # Cannot use the same relu layer as the quantization expects the relu to be fused with the conv layer\n",
    "        \n",
    "        self.maxpool_pad = nn.MaxPool2d(2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        \n",
    "        self.load_state_dict(torch.load(weight_path))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.maxpool_pad(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dequant(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.forward(x), 1)\n",
    "        \n",
    "model_fp32 = MNISTClassificationQuantize('checkpoints/model_0_0.pth')\n",
    "\n",
    "model_fp32.eval()\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(\n",
    "    model_fp32, [\n",
    "        ['conv1', 'relu1'], \n",
    "        ['conv2', 'relu2'], \n",
    "        ['conv3', 'relu3']\n",
    "        ])\n",
    "model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "inp_fp32 = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "model_fp32_prepared(inp_fp32)\n",
    "\n",
    "model_int8_quantized_ptsq = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "res = model_int8_quantized_ptsq(inp_fp32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_fp32,\n",
    "    # torch.randn(1, 1, 28, 28).cuda() if torch.cuda.is_available() else torch.randn(1, 1, 28, 28),\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_fp32_additional.onnx',\n",
    "    export_params=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_int8_quantized_ptsq,\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_fp32_quantized.onnx',\n",
    "    export_params=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT - Quantization Aware Training\n",
    "\n",
    " - As with the post-training quantization, I will use the PyTorch quantization tools to perform the quantization aware training.\n",
    " - The quantization aware training is best done as a fine-tune step rather than training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Finetune Epoch : 0\n",
      "Accuracy: 0.99025\n",
      "QAT Finetune Epoch : 1\n",
      "Accuracy: 0.99025\n",
      "QAT Finetune Epoch : 2\n",
      "Accuracy: 0.99025\n",
      "QAT Finetune Epoch : 3\n",
      "Accuracy: 0.99025\n",
      "QAT Finetune Epoch : 4\n",
      "Accuracy: 0.99025\n",
      "Best Model : \n",
      "{\n",
      "    \"accuracy\": 0,\n",
      "    \"epoch\": 0,\n",
      "    \"fold\": 4,\n",
      "    \"training_acc\": -0.98052978515625,\n",
      "    \"val_acc\": 0.99025\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Quantization Aware Training\n",
    "\n",
    "model_fp32 = MNISTClassificationQuantize('checkpoints/best_model.pth')\n",
    "model_fp32.train()\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(\n",
    "    model_fp32, [\n",
    "        ['conv1', 'relu1'], \n",
    "        ['conv2', 'relu2'], \n",
    "        ['conv3', 'relu3']\n",
    "        ])\n",
    "model_fp32_qat_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "qat_best_params = {\n",
    "    'model': None,\n",
    "    'accuracy': 0,\n",
    "    'epoch': 0,\n",
    "    'fold': 0,\n",
    "    'training_acc': None,\n",
    "    'val_acc': None\n",
    "}\n",
    "\n",
    "qat_device = torch.device('cpu') # Choosing CPU for now\n",
    "\n",
    "# K Fold not needed as we are using the best model\n",
    "for epoch in range(QUANTIZATION_TRAIN_EPOCHS):\n",
    "    print(f\"QAT Finetune Epoch : {epoch}\")\n",
    "    loss_val = train(model_fp32_qat_prepared, train_loader, loss_fn, device=qat_device)\n",
    "    val_loss = validate(model_fp32_qat_prepared, test_loader, loss_fn=loss_fn, device=qat_device)\n",
    "    val_acc = test_acc(model_fp32_qat_prepared, test_loader, device=qat_device)\n",
    "    \n",
    "    if qat_best_params['val_acc'] is None or val_acc > qat_best_params['val_acc'] :\n",
    "        qat_best_params['model'] = model_fp32_qat_prepared\n",
    "        qat_best_params['epoch'] = epoch\n",
    "        qat_best_params['fold'] = fold\n",
    "        qat_best_params['training_loss'] = loss_val\n",
    "        qat_best_params['val_loss'] = val_loss\n",
    "        qat_best_params['val_acc'] = val_acc\n",
    "        \n",
    "print(\"Best Model : \")\n",
    "print(json.dumps({k:qat_best_params[k] for k in qat_best_params.keys() if 'model' not in k}, indent=4))\n",
    "\n",
    "# Save the best model\n",
    "torch.save(qat_best_params['model'].state_dict(), f\"checkpoints/qat_best_model.pth\")\n",
    "\n",
    "model_fp32_qat_prepared.eval()\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_qat_prepared)\n",
    "\n",
    "inp_fp32 = torch.randn(1, 1, 28, 28)\n",
    "res = model_int8(inp_fp32)\n",
    "\n",
    "# Export ONNX\n",
    "torch.onnx.export(\n",
    "    model_int8,\n",
    "    torch.randn(1, 1, 28, 28),\n",
    "    'model_int8.onnx',\n",
    "    export_params=True,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Model size :  1391.5390625 Kbytes\n",
      "Quantized INT8 Model size :  348.609375 Kbytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import torchinfo\n",
    "# torchinfo.summary(model_fp32_quantized, input_size=(1, 1, 28, 28))\n",
    "\n",
    "# TODO: Calculate the size of the model\n",
    "\n",
    "def calculate_model_size(model : nn.Module):\n",
    "    model_size = 0\n",
    "    for param in model.parameters():    \n",
    "        model_size += param.numel() * param.element_size()\n",
    "    return model_size\n",
    "\n",
    "def calculate_quantized_model_size(model):\n",
    "    # This only considers the weights and not the biases. That is why the size is smaller\n",
    "    # than the model file size.\n",
    "    model_int8_size = 0\n",
    "    weights = model.state_dict()\n",
    "    for key in weights:\n",
    "        if 'weight' in key:\n",
    "            weight_tensor : torch.Tensor = weights[key]\n",
    "            model_int8_size += weight_tensor.numel() * weight_tensor.element_size()\n",
    "            \n",
    "        if ('packed_params' in key and 'dtype' not in key):\n",
    "            for ele in weights[key]:\n",
    "                if isinstance(ele, torch.Tensor):\n",
    "                    model_int8_size += ele.numel() * ele.element_size()\n",
    "                if isinstance(ele, torch.nn.Parameter):\n",
    "                    model_int8_size += ele.numel() * ele.element_size()\n",
    "        # model_int8_size += weights[key].numel() * weights[key].element_size()\n",
    "    return model_int8_size\n",
    "\n",
    "\n",
    "model_fp32_size = calculate_model_size(model_fp32)\n",
    "model_int8_quantized_ptsq_size = calculate_quantized_model_size(model_int8_quantized_ptsq)\n",
    "model_int8_size = calculate_quantized_model_size(model_int8)\n",
    "\n",
    "print(\"FP32 Model size : \", model_fp32_size / 1024, \"Kbytes\")\n",
    "print(\"Quantized INT8 Model size : \", model_int8_size / 1024, \"Kbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9796666666666667\n",
      "Accuracy: 0.4046666666666667\n",
      "Accuracy: 0.9899166666666667\n",
      "FP32 Accuracy :  0.9796666666666667\n",
      "INT8 PTSQ Accuracy :  0.4046666666666667\n",
      "INT8 QAT Accuracy :  0.9899166666666667\n",
      "PTSQ Loss in Accuracy :  58.693433140523986 %\n",
      "QAT Loss in Accuracy :  -1.0462742429397736 %\n"
     ]
    }
   ],
   "source": [
    "# Accuracy comparison\n",
    "\n",
    "fp32_acc = test_acc(model, test_loader, device)\n",
    "int8_ptsq_acc = test_acc(model_int8_quantized_ptsq, test_loader, torch.device('cpu'))\n",
    "int8_qat_acc = test_acc(model_int8, test_loader, torch.device('cpu'))\n",
    "\n",
    "print(\"FP32 Accuracy : \", fp32_acc)\n",
    "print(\"INT8 PTSQ Accuracy : \", int8_ptsq_acc)\n",
    "print(\"INT8 QAT Accuracy : \", int8_qat_acc)\n",
    "\n",
    "print(\"PTSQ Loss in Accuracy : \", ((fp32_acc - int8_ptsq_acc )/ fp32_acc) * 100, \"%\")\n",
    "print(\"QAT Loss in Accuracy : \", ((fp32_acc - int8_qat_acc )/ fp32_acc) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
